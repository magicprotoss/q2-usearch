{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiime2 import Artifact\n",
    "\n",
    "reference_reads = Artifact.load('/mnt/amplicon-db/gtdb-207/seqs.qza')\n",
    "reference_taxonomy = Artifact.load('/mnt/amplicon-db/gtdb-207/taxa.qza')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import skbio\n",
    "\n",
    "\n",
    "def _convert_q2_seqs_and_taxa_to_utax(working_dir, reference_reads, reference_taxonomy):\n",
    "    ref_reads_se = reference_reads.view(pd.Series)\n",
    "    ref_reads_se.name = 'Seqs'\n",
    "    ref_reads_se.index.name = 'Feature ID'\n",
    "    ref_taxa_df = reference_taxonomy.view(pd.DataFrame)\n",
    "    tmp_taxa_map_df = _make_tmp_tax_mapping_df(ref_taxa_df)\n",
    "    \n",
    "    op_fa = os.path.join(working_dir, 'silva_sintax_fmt.fa')\n",
    "    with open(op_fa, 'wt') as fh:\n",
    "        for index, item in ref_reads_se.items():\n",
    "            tax_info = tmp_taxa_map_df.at[index, 'usearch_tax']\n",
    "            seq_to_dump = skbio.DNA(str(item).upper(), metadata = {'id': index + tax_info})\n",
    "            seq_to_dump.write(fh, format=\"fasta\", max_width=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_convert_q2_seqs_and_taxa_to_utax('/home/navi/synonas/My_Testing_Ground/sintax_attempt_2', reference_reads, reference_taxonomy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_tax_into_ranks(tax, sep):\n",
    "    tax_lst = str(tax).split(sep)\n",
    "    return tax_lst\n",
    "\n",
    "\n",
    "def _split_tax_into_ranks_and_get_max_levels(tax_df_in, sep):\n",
    "    # need to confirm if sintax accepts non-7-rank systems\n",
    "    tax_rank_split_se = tax_df_in['Taxon'].apply(_split_tax_into_ranks, sep=sep)\n",
    "    max_levels = tax_rank_split_se.apply(len).max()\n",
    "    if max_levels > 7:\n",
    "        raise KeyError('according to the doc, sintax only supports up to 7 levels')\n",
    "    return max_levels, tax_rank_split_se\n",
    "\n",
    "\n",
    "def _replace_q2_split_w_usearch_split_and_remove_leading_trailing_blanks(rank_in):\n",
    "    rank_out = re.sub(r\"(?<=\\b[dpcofgs])\\w*__\", ':', str(rank_in).strip())\n",
    "    return rank_out\n",
    "\n",
    "\n",
    "def _replace_non_7bit_ascii_chars(rank_in):\n",
    "    rank_out = rank_in\n",
    "    for index, tax_char in enumerate(rank_in):\n",
    "        if ord(tax_char) > 127:\n",
    "            rank_out = rank_in[:index] + str(index) + rank_in[index + 1:]\n",
    "    return rank_out\n",
    "\n",
    "\n",
    "def _detect_empty_ph_ranks(rank_in):\n",
    "    result = False\n",
    "    if rank_in == np.nan:\n",
    "        result = True\n",
    "    else:\n",
    "        pattern = r\"^[dpcofgs]:$\"\n",
    "        result = bool(re.match(pattern, rank_in))\n",
    "    return result\n",
    "\n",
    "\n",
    "def _join_levels_for_usearch(tax_series_in):\n",
    "    # This pile of 💩 is here for one reason:\n",
    "    # if a non authoritive database contains a place holder in a parent rank and also have a non-empty child rank\n",
    "    usearch_tax_anno_str = ';tax='\n",
    "    for level, (index, rank) in enumerate(tax_series_in.items(), 1):\n",
    "        detection_res = _detect_empty_ph_ranks(rank)\n",
    "        if detection_res:\n",
    "            break\n",
    "        else:\n",
    "            if level == 1:\n",
    "                usearch_tax_anno_str = usearch_tax_anno_str + rank\n",
    "            else:\n",
    "                usearch_tax_anno_str = usearch_tax_anno_str + ',' + rank\n",
    "    usearch_tax_anno_str = usearch_tax_anno_str + ';'\n",
    "    return usearch_tax_anno_str\n",
    "\n",
    "\n",
    "def _make_tmp_tax_mapping_df(tax_df_in):\n",
    "\n",
    "    tax_df_out = tax_df_in.copy()\n",
    "\n",
    "    max_level, tax_rank_split_se = _split_tax_into_ranks_and_get_max_levels(\n",
    "        tax_df_in, ';')\n",
    "\n",
    "    ori_tax_cols_lst = ['q2_' + 'level' + '_' + str(i) for i in range(1, max_level + 1)]\n",
    "    u_tax_cols_lst = ['usearch_' + 'level' + '_' +\n",
    "                      str(i) for i in range(1, max_level + 1)]\n",
    "\n",
    "    # 💩 super slow but works\n",
    "    tax_df_out[ori_tax_cols_lst] = tax_rank_split_se.apply(pd.Series)\n",
    "\n",
    "    tax_df_out[u_tax_cols_lst] = tax_df_out[ori_tax_cols_lst]\n",
    "\n",
    "    for u_level in u_tax_cols_lst:\n",
    "        tax_df_out[u_level] = tax_df_out[u_level].apply(\n",
    "            _replace_q2_split_w_usearch_split_and_remove_leading_trailing_blanks)\n",
    "        tax_df_out[u_level] = tax_df_out[u_level].apply(_replace_non_7bit_ascii_chars)\n",
    "    tax_df_out['usearch_tax'] = tax_df_out[u_tax_cols_lst].apply(\n",
    "        _join_levels_for_usearch, axis=1)\n",
    "\n",
    "    return tax_df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_df_out = _make_tmp_tax_mapping_df(reference_taxonomy.view(pd.DataFrame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord(\"ë\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Taxon'].apply(_split_q2_tax_into_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.loc['GB_GCA_000006155.2', 'level7'] = np.nan\n",
    "test.loc['GB_GCA_000006155.2', ['level1', 'level2', 'level3', 'level4', 'level5', 'level6', 'level7']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_join_levels_for_usearch(['d:Bacteria',\n",
    " 'p:Firmicutes',\n",
    " 'c:Bacilli',\n",
    " 'o:Bacillales',\n",
    " 'f:Bacillaceae_G',\n",
    " 'g:',\n",
    " 's:noob silva db'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_replace_non_7bit_ascii_chars(\"sssë\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_taxa_df = reference_taxonomy.view(pd.DataFrame)\n",
    "ref_taxa_df = ref_taxa_df.iloc[0:10, :]\n",
    "\n",
    "ref_taxa_df[['Domain', 'Phylum', 'Class', 'Order', 'Family', 'Genus', 'Species']] = ref_taxa_df['Taxon'].str.split(';', expand = True)\n",
    "\n",
    "for index, row in ref_taxa_df.iterrows():\n",
    "    if index == 'EF096034.1.1347':\n",
    "        for index_se, item in row.items():\n",
    "            print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_taxa_df[['Domain', 'Phylum']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test merge tax\n",
    "\n",
    "def _map_utax_to_q2_tax(tax_rank_split_in, taxa_map_df):\n",
    "    # split input and map into sep dfs, left join and concat back\n",
    "    levels = len(tax_rank_split_in.columns)\n",
    "    tax_rank_split_mapped_to_q2_tax = tax_rank_split_in.copy()\n",
    "    for i in range(1, levels + 1):\n",
    "        key = \"level_\" + str(i)\n",
    "        taxa_map_df_sub_level = taxa_map_df[['usearch_' + key, 'q2_' + key]]\n",
    "        taxa_map_df_sub_level_uni = taxa_map_df_sub_level.drop_duplicates().dropna()\n",
    "        taxa_map_df_sub_level_uni_dict = taxa_map_df_sub_level_uni.set_index('usearch_' + key).iloc[:, 0].to_dict()\n",
    "        tax_rank_split_mapped_to_q2_tax['usearch_' + key] = tax_rank_split_in['usearch_' + key].replace(taxa_map_df_sub_level_uni_dict)\n",
    "    return tax_rank_split_mapped_to_q2_tax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_tax_into_ranks(tax, sep):\n",
    "    tax_lst = str(tax).split(sep)\n",
    "    return tax_lst\n",
    "\n",
    "\n",
    "def _split_tax_into_ranks_and_get_max_levels(tax_df_in, sep):\n",
    "    # need to confirm if sintax accepts non-7-rank systems\n",
    "    tax_rank_split_se = tax_df_in['Taxon'].apply(_split_tax_into_ranks, sep=sep)\n",
    "    max_levels = tax_rank_split_se.apply(len).max()\n",
    "    if max_levels > 7:\n",
    "        raise KeyError('according to the doc, sintax only supports up to 7 levels')\n",
    "    return max_levels, tax_rank_split_se\n",
    "\n",
    "def _rm_conf_value_and_trim_fp_ranks(x, cut_off=float):\n",
    "    if x == None:\n",
    "        x_str = np.nan\n",
    "    else:\n",
    "        sintan_conf_col_loci = x.rfind('(')\n",
    "        x_str = x[:sintan_conf_col_loci]\n",
    "        x_conf = float(x[sintan_conf_col_loci + 1:].replace(')', ''))\n",
    "        if x_conf < cut_off:\n",
    "            x_str = np.nan\n",
    "    return x_str\n",
    "\n",
    "\n",
    "def _get_conf_value_deepest_rank(se_in, cut_off=float):\n",
    "\n",
    "    conf = np.nan\n",
    "    for index, item in se_in.items():\n",
    "        sintan_conf_col_loci = item.rfind('(')\n",
    "        conf_item = float(item[sintan_conf_col_loci + 1:].replace(')', ''))\n",
    "        if conf_item < cut_off:\n",
    "            break\n",
    "        conf = conf_item\n",
    "\n",
    "    return conf\n",
    "\n",
    "\n",
    "def _split_utax_and_get_conf_lr(usearch_tax_df_in, confidence):\n",
    "\n",
    "    tax_rank_split_df = pd.DataFrame(index=usearch_tax_df_in.index)\n",
    "\n",
    "    max_level, usearch_tax_rank_split_df = _split_tax_into_ranks_and_get_max_levels(\n",
    "        usearch_tax_df_in, ',')\n",
    "\n",
    "    u_tax_cols_lst = ['usearch_' + 'level' + '_' +\n",
    "                      str(i) for i in range(1, max_level + 1)]\n",
    "\n",
    "    # 💩 super slow but works\n",
    "    tax_rank_split_df[u_tax_cols_lst] = usearch_tax_rank_split_df.apply(pd.Series)\n",
    "\n",
    "    id_conf_df = pd.DataFrame(index=usearch_tax_df_in.index)\n",
    "\n",
    "    for index, row in tax_rank_split_df.iterrows():\n",
    "        id_conf_df.at[index, 'Confidence'] = _get_conf_value_deepest_rank(\n",
    "            row, confidence)\n",
    "\n",
    "    tax_rank_split_df = tax_rank_split_df.applymap(\n",
    "        _rm_conf_value_and_trim_fp_ranks, cut_off=confidence)\n",
    "\n",
    "    return tax_rank_split_df, id_conf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usearch_tax = pd.read_csv(os.path.join(\n",
    "    '/home/navi/synonas/My_Testing_Ground/sintax_attempt_2', 'sintax.tsv'), sep='\\t', header=None, index_col=0)\n",
    "\n",
    "usearch_tax.index.name = 'Feature ID'\n",
    "\n",
    "usearch_tax = usearch_tax.iloc[:, 0:2]\n",
    "\n",
    "usearch_tax.columns = ['Taxon', 'Strand']\n",
    "\n",
    "# if both strand were used, split into plus_tax and minus df them comp\n",
    "# improve later, since amp reads always ends in plus strand\n",
    "usearch_tax = usearch_tax['Taxon'].to_frame()\n",
    "usearch_tax\n",
    "\n",
    "tax_rank_split_df, id_conf_df = _split_utax_and_get_conf_lr(usearch_tax, 0.7)\n",
    "tax_rank_split_df\n",
    "# id_conf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_rank_split_df.at['87905bbe46f2db94cd2a31d83a765233;size=2745;', 'usearch_level_7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = _map_utax_to_q2_tax(tax_rank_split_df, tax_df_out)\n",
    "test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _join_q2_tax(q2_tax_rank_split_in):\n",
    "    q2_tax = pd.DataFrame()\n",
    "    for index, row in q2_tax_rank_split_in.iterrows():\n",
    "        tax_str = '; '.join(row.dropna().values)\n",
    "        if len(tax_str) == 0:\n",
    "            tax_str = 'Unclassified'\n",
    "        q2_tax.at[index, 'Taxon'] = tax_str\n",
    "    q2_tax.index.name = 'Feature ID'\n",
    "\n",
    "    return q2_tax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'; '.join([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_df_out.at['EF096034.1.1347', 'q2_level_1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep Final test here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import skbio\n",
    "import subprocess\n",
    "import tempfile\n",
    "import hashlib\n",
    "\n",
    "\n",
    "def run_commands(cmds, verbose=True):\n",
    "    if verbose:\n",
    "        print(\"Running external command line application(s). This may print \"\n",
    "              \"messages to stdout and/or stderr.\")\n",
    "        print(\"The command(s) being run are below. These commands cannot \"\n",
    "              \"be manually re-run as they will depend on temporary files that \"\n",
    "              \"no longer exist.\")\n",
    "    for cmd in cmds:\n",
    "        if verbose:\n",
    "            print(\"\\nCommand:\", end=' ')\n",
    "            print(\" \".join(cmd), end='\\n\\n')\n",
    "        subprocess.run(cmd, check=True)\n",
    "\n",
    "\n",
    "def _get_input_seqs_ids_and_dump_to_fasta(working_dir, query_se):\n",
    "    with open(os.path.join(working_dir, 'query.fasta'), 'wt') as fh:\n",
    "        for index, item in query_se.items():\n",
    "            item.write(fh, format=\"fasta\", max_width=80)\n",
    "    empty_df_w_input_seqs_labs = pd.DataFrame(index=query_se.index)\n",
    "    return empty_df_w_input_seqs_labs\n",
    "\n",
    "\n",
    "def _split_tax_into_ranks(tax, sep):\n",
    "    tax_lst = str(tax).split(sep)\n",
    "    return tax_lst\n",
    "\n",
    "\n",
    "def _split_tax_into_ranks_and_get_max_levels(tax_df_in, sep):\n",
    "    # need to confirm if sintax accepts non-7-rank systems\n",
    "    tax_rank_split_se = tax_df_in['Taxon'].apply(_split_tax_into_ranks, sep=sep)\n",
    "    max_levels = tax_rank_split_se.apply(len).max()\n",
    "    if max_levels > 7:\n",
    "        raise KeyError('according to the doc, sintax only supports up to 7 levels')\n",
    "    return max_levels, tax_rank_split_se\n",
    "\n",
    "\n",
    "def _replace_q2_split_w_usearch_split_and_remove_leading_trailing_blanks(rank_in):\n",
    "    rank_out = re.sub(r\"(?<=\\b[dpcofgs])\\w*__\", ':', str(rank_in).strip())\n",
    "    return rank_out\n",
    "\n",
    "\n",
    "def _replace_non_7bit_ascii_chars(rank_in):\n",
    "    rank_out = rank_in\n",
    "\n",
    "    # let's KISS here...\n",
    "    if re.search(r'[^A-Za-z0-9_.]', rank_in):\n",
    "        if re.match(r'^[kdpcofgs]', str(rank_in).strip()):\n",
    "            rank_out = rank_in[0] + ':' + \\\n",
    "                hashlib.md5(str(rank_in).encode('utf-8')).hexdigest()\n",
    "        else:\n",
    "            rank_out = np.nan\n",
    "    return rank_out\n",
    "\n",
    "\n",
    "def _detect_empty_ph_ranks(rank_in):\n",
    "    result = False\n",
    "    if rank_in == np.nan:\n",
    "        result = True\n",
    "    else:\n",
    "        pattern = r\"^[dpcofgs]:$\"\n",
    "        result = bool(re.match(pattern, rank_in))\n",
    "    return result\n",
    "\n",
    "\n",
    "def _join_levels_for_usearch(tax_series_in):\n",
    "    # This pile of 💩 is here for one reason:\n",
    "    # if a non authoritive database contains a place holder in a parent rank and also have a non-empty child rank\n",
    "    usearch_tax_anno_str = ';tax='\n",
    "    for level, (index, rank) in enumerate(tax_series_in.items(), 1):\n",
    "        detection_res = _detect_empty_ph_ranks(rank)\n",
    "        if detection_res:\n",
    "            break\n",
    "        else:\n",
    "            if level == 1:\n",
    "                usearch_tax_anno_str = usearch_tax_anno_str + rank\n",
    "            else:\n",
    "                usearch_tax_anno_str = usearch_tax_anno_str + ',' + rank\n",
    "    usearch_tax_anno_str = usearch_tax_anno_str + ';'\n",
    "    return usearch_tax_anno_str\n",
    "\n",
    "\n",
    "def _make_tmp_tax_mapping_df(tax_df_in):\n",
    "\n",
    "    tax_df_out = tax_df_in.copy()\n",
    "\n",
    "    max_level, tax_rank_split_se = _split_tax_into_ranks_and_get_max_levels(\n",
    "        tax_df_in, ';')\n",
    "\n",
    "    ori_tax_cols_lst = ['q2_' + 'level' + '_' + str(i) for i in range(1, max_level + 1)]\n",
    "    u_tax_cols_lst = ['usearch_' + 'level' + '_' +\n",
    "                      str(i) for i in range(1, max_level + 1)]\n",
    "\n",
    "    # 💩 super slow but works\n",
    "    tax_df_out[ori_tax_cols_lst] = tax_rank_split_se.apply(pd.Series)\n",
    "\n",
    "    tax_df_out[u_tax_cols_lst] = tax_df_out[ori_tax_cols_lst]\n",
    "\n",
    "    for u_level in u_tax_cols_lst:\n",
    "        tax_df_out[u_level] = tax_df_out[u_level].apply(\n",
    "            _replace_q2_split_w_usearch_split_and_remove_leading_trailing_blanks)\n",
    "        tax_df_out[u_level] = tax_df_out[u_level].apply(_replace_non_7bit_ascii_chars)\n",
    "    tax_df_out['usearch_tax'] = tax_df_out[u_tax_cols_lst].apply(\n",
    "        _join_levels_for_usearch, axis=1)\n",
    "\n",
    "    return tax_df_out\n",
    "\n",
    "\n",
    "def _convert_q2_seqs_and_taxa_to_utax(working_dir, reference_reads, reference_taxonomy, verbose):\n",
    "    ref_reads_se = reference_reads\n",
    "    ref_reads_se.name = 'Seqs'\n",
    "    ref_reads_se.index.name = 'Feature ID'\n",
    "    ref_taxa_df = reference_taxonomy\n",
    "    # check if dumping tax_df to pickle is nessesary with low spec pcs\n",
    "    # silva 138.1 only took 72m mem, no need here\n",
    "    if verbose:\n",
    "        print(\"Building usearch compatible fasta db file, this could take a while...\")\n",
    "\n",
    "    tmp_taxa_map_df = _make_tmp_tax_mapping_df(ref_taxa_df)\n",
    "\n",
    "    op_fa = os.path.join(working_dir, 'ref_seqs_tax.fa')\n",
    "    with open(op_fa, 'wt') as fh:\n",
    "        for index, item in ref_reads_se.items():\n",
    "            tax_info = tmp_taxa_map_df.at[index, 'usearch_tax']\n",
    "            seq_to_dump = skbio.DNA(str(item).upper(), metadata={\n",
    "                                    'id': index + tax_info})\n",
    "            seq_to_dump.write(fh, format=\"fasta\", max_width=80)\n",
    "\n",
    "    return tmp_taxa_map_df\n",
    "\n",
    "\n",
    "def _build_udb():\n",
    "    # seemed unnessasary, sintax builds one on the fly, and tmp dirs don't presist in a q2 pipeline\n",
    "    pass\n",
    "\n",
    "\n",
    "def _run_sintax(working_dir, query_seqs_fp, strand, threads, verbose):\n",
    "    # build sintax command\n",
    "    cmd = ['usearch', '-sintax', query_seqs_fp, '-db', os.path.join(\n",
    "        working_dir, 'ref_seqs_tax.fa'), '-tabbedout', os.path.join(working_dir, 'sintax.tsv')]\n",
    "\n",
    "    if strand == 'plus':\n",
    "        cmd += ['-strand', 'plus']\n",
    "    else:\n",
    "        cmd += ['-strand', 'both']\n",
    "\n",
    "    if threads != 1:\n",
    "        cmd += ['-threads', str(threads)]\n",
    "\n",
    "    run_commands([cmd])\n",
    "\n",
    "\n",
    "def _rm_conf_value_and_trim_fp_ranks(x, cut_off=float):\n",
    "    if x == None:\n",
    "        x_str = np.nan\n",
    "    else:\n",
    "        sintan_conf_col_loci = x.rfind('(')\n",
    "        x_str = x[:sintan_conf_col_loci]\n",
    "        x_conf = float(x[sintan_conf_col_loci + 1:].replace(')', ''))\n",
    "        if x_conf < cut_off:\n",
    "            x_str = np.nan\n",
    "    return x_str\n",
    "\n",
    "\n",
    "def _get_conf_value_deepest_rank(se_in, cut_off=float):\n",
    "\n",
    "    conf = np.nan\n",
    "    for index, item in se_in.items():\n",
    "        sintan_conf_col_loci = item.rfind('(')\n",
    "        conf_item = float(item[sintan_conf_col_loci + 1:].replace(')', ''))\n",
    "        if conf_item < cut_off:\n",
    "            break\n",
    "        conf = conf_item\n",
    "\n",
    "    return conf\n",
    "\n",
    "\n",
    "def _split_utax_and_get_conf_lr(usearch_tax_df_in, confidence):\n",
    "\n",
    "    tax_rank_split_df = pd.DataFrame(index=usearch_tax_df_in.index)\n",
    "\n",
    "    max_level, usearch_tax_rank_split_df = _split_tax_into_ranks_and_get_max_levels(\n",
    "        usearch_tax_df_in, ',')\n",
    "\n",
    "    u_tax_cols_lst = ['usearch_' + 'level' + '_' +\n",
    "                      str(i) for i in range(1, max_level + 1)]\n",
    "\n",
    "    # 💩 super slow but works\n",
    "    tax_rank_split_df[u_tax_cols_lst] = usearch_tax_rank_split_df.apply(pd.Series)\n",
    "\n",
    "    id_conf_df = pd.DataFrame(index=usearch_tax_df_in.index)\n",
    "\n",
    "    for index, row in tax_rank_split_df.iterrows():\n",
    "        id_conf_df.at[index, 'Confidence'] = _get_conf_value_deepest_rank(\n",
    "            row, confidence)\n",
    "\n",
    "    tax_rank_split_df = tax_rank_split_df.applymap(\n",
    "        _rm_conf_value_and_trim_fp_ranks, cut_off=confidence)\n",
    "\n",
    "    return tax_rank_split_df, id_conf_df\n",
    "\n",
    "\n",
    "def _map_utax_to_q2_tax(tax_rank_split_in, taxa_map_df):\n",
    "    # split input and map into sep dfs, left join and concat back\n",
    "    levels = len(tax_rank_split_in.columns)\n",
    "    tax_rank_split_mapped_to_q2_tax = tax_rank_split_in.copy()\n",
    "    for i in range(1, levels + 1):\n",
    "        key = \"level_\" + str(i)\n",
    "        taxa_map_df_sub_level = taxa_map_df[['usearch_' + key, 'q2_' + key]]\n",
    "        taxa_map_df_sub_level_uni = taxa_map_df_sub_level.drop_duplicates().dropna()\n",
    "        taxa_map_df_sub_level_uni_dict = taxa_map_df_sub_level_uni.set_index(\n",
    "            'usearch_' + key).iloc[:, 0].to_dict()\n",
    "        tax_rank_split_mapped_to_q2_tax['usearch_' + key] = tax_rank_split_in['usearch_' + key].replace(\n",
    "            taxa_map_df_sub_level_uni_dict)\n",
    "    return tax_rank_split_mapped_to_q2_tax\n",
    "\n",
    "\n",
    "def _join_q2_tax(q2_tax_rank_split_in):\n",
    "    q2_tax = pd.DataFrame()\n",
    "    for index, row in q2_tax_rank_split_in.iterrows():\n",
    "        tax_str = '; '.join(row.dropna().values)\n",
    "        if len(tax_str) == 0:\n",
    "            tax_str = 'Unclassified'\n",
    "        tmp_df = pd.DataFrame({'Taxon': {index: tax_str}})\n",
    "        q2_tax = pd.concat([q2_tax, tmp_df])\n",
    "    q2_tax.index.name = 'Feature ID'\n",
    "\n",
    "    return q2_tax\n",
    "\n",
    "\n",
    "def _collect_sintax_anno_to_q2_anno(working_dir, taxa_map_df, empty_df_w_input_seqs_labs, strand, confidence, verbose):\n",
    "\n",
    "    # read sintax res into pd.DataFrame\n",
    "    usearch_tax = pd.read_csv(os.path.join(\n",
    "        working_dir, 'sintax.tsv'), sep='\\t', header=None, index_col=0)\n",
    "\n",
    "    usearch_tax.index.name = 'Feature ID'\n",
    "\n",
    "    usearch_tax = usearch_tax.iloc[:, 0:2]\n",
    "\n",
    "    usearch_tax.columns = ['Taxon', 'Strand']\n",
    "\n",
    "    # rewrite in a less 💩 way here later...\n",
    "    if strand != 'plus':\n",
    "        usearch_tax_both = {}\n",
    "        usearch_tax_both['plus'] = usearch_tax.loc[usearch_tax['Strand']\n",
    "                                                   == '+', 'Taxon'].to_frame()\n",
    "        usearch_tax_both['minus'] = usearch_tax.loc[usearch_tax['Strand']\n",
    "                                                    == '-', 'Taxon'].to_frame()\n",
    "        usearch_tax_rank_split_dfs = {}\n",
    "        id_conf_dfs = {}\n",
    "        q2_tax_rank_splits = {}\n",
    "        q2_taxs = {}\n",
    "        for key, value in usearch_tax_both.items():\n",
    "\n",
    "            usearch_tax_rank_split_dfs[key], id_conf_dfs[key] = _split_utax_and_get_conf_lr(\n",
    "                value, confidence)\n",
    "\n",
    "            q2_tax_rank_splits[key] = _map_utax_to_q2_tax(\n",
    "                usearch_tax_rank_split_dfs[key], taxa_map_df)\n",
    "\n",
    "            q2_taxs[key] = _join_q2_tax(q2_tax_rank_splits[key])\n",
    "\n",
    "        q2_tax = _comp_plus_minus_res_and_opt_final_res(empty_df_w_input_seqs_labs, q2_taxs, q2_tax_rank_splits, id_conf_dfs)\n",
    "\n",
    "    else:\n",
    "        usearch_tax = usearch_tax['Taxon'].to_frame()\n",
    "\n",
    "        usearch_tax_rank_split_df, id_conf_df = _split_utax_and_get_conf_lr(\n",
    "            usearch_tax, confidence)\n",
    "\n",
    "        q2_tax_rank_split = _map_utax_to_q2_tax(usearch_tax_rank_split_df, taxa_map_df)\n",
    "\n",
    "        q2_tax = _join_q2_tax(q2_tax_rank_split)\n",
    "\n",
    "        q2_tax = pd.merge(q2_tax, id_conf_df, left_index=True,\n",
    "                          right_index=True, how='inner')\n",
    "\n",
    "        q2_tax = pd.merge(empty_df_w_input_seqs_labs, q2_tax,\n",
    "                          left_index=True, right_index=True, how='left')\n",
    "\n",
    "        for index, row in q2_tax.iterrows():\n",
    "            if row['Taxon'] is None:\n",
    "                q2_tax.at[index, 'Taxon'] == 'Unclassified'\n",
    "\n",
    "    return q2_taxs\n",
    "\n",
    "\n",
    "def sintax(query: pd.Series,\n",
    "           reference_reads: pd.Series,\n",
    "           reference_taxonomy: pd.DataFrame,\n",
    "           strand: str = 'plus',\n",
    "           threads: str = \"auto\",\n",
    "           confidence: float = 0.8\n",
    "           ) -> (pd.DataFrame):\n",
    "\n",
    "    verbose = True\n",
    "\n",
    "    if threads == \"auto\":\n",
    "        threads = os.cpu_count() - 3\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as usearch_wd:\n",
    "\n",
    "        empty_df_w_input_seqs_labs = _get_input_seqs_ids_and_dump_to_fasta(\n",
    "            usearch_wd, query)\n",
    "\n",
    "        reference_reads = reference_reads.view()\n",
    "\n",
    "        taxa_map_df = _convert_q2_seqs_and_taxa_to_utax(\n",
    "            usearch_wd, reference_reads, reference_taxonomy, verbose)\n",
    "\n",
    "        query_fp = os.path.join(usearch_wd, 'query.fasta')\n",
    "\n",
    "        _run_sintax(usearch_wd, query_fp, strand, threads, verbose)\n",
    "\n",
    "        classification = _collect_sintax_anno_to_q2_anno(\n",
    "            usearch_wd, taxa_map_df, empty_df_w_input_seqs_labs, strand, confidence, verbose)\n",
    "\n",
    "    return classification\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_conf_dfs_test = {}\n",
    "q2_tax_rank_splits_test = {}\n",
    "q2_taxs_test = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building usearch compatible fasta db file, this could take a while...\n",
      "Running external command line application(s). This may print messages to stdout and/or stderr.\n",
      "The command(s) being run are below. These commands cannot be manually re-run as they will depend on temporary files that no longer exist.\n",
      "\n",
      "Command: usearch -sintax /tmp/tmpaukmy69x/query.fasta -db /tmp/tmpaukmy69x/ref_seqs_tax.fa -tabbedout /tmp/tmpaukmy69x/sintax.tsv -strand both -threads 21\n",
      "\n",
      "usearch v11.0.667_i86linux64, 132Gb RAM, 24 cores\n",
      "(C) Copyright 2013-18 Robert C. Edgar, all rights reserved.\n",
      "https://drive5.com/usearch\n",
      "\n",
      "License: yxliu@genetics.ac.cn, non-profit use, max 1 process(es)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:00 108Mb   100.0% Reading /tmp/tmpaukmy69x/ref_seqs_tax.fa\n",
      "00:00 74Mb    100.0% Masking (fastnucleo)                    \n",
      "00:01 75Mb    100.0% Word stats          \n",
      "00:01 75Mb    100.0% Alloc rows\n",
      "00:03 269Mb   100.0% Build index\n",
      "00:04 1.8Gb   100.0% Processing \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'plus':                                                         Taxon\n",
       " Feature ID                                                   \n",
       " ASV5        d__Bacteria; p__Bacteroidota; c__Bacteroidia; ...\n",
       " ASV6        d__Bacteria; p__Bacteroidota; c__Bacteroidia; ...\n",
       " ASV9        d__Bacteria; p__Bacteroidota; c__Bacteroidia; ...\n",
       " ASV4        d__Bacteria; p__Proteobacteria; c__Alphaproteo...\n",
       " ASV11       d__Bacteria; p__Proteobacteria; c__Gammaproteo...\n",
       " ...                                                       ...\n",
       " ASV492      d__Bacteria; p__Proteobacteria; c__Alphaproteo...\n",
       " ASV500      d__Bacteria; p__Bacteroidota; c__Bacteroidia; ...\n",
       " ASV499      d__Bacteria; p__Proteobacteria; c__Alphaproteo...\n",
       " ASV498      d__Bacteria; p__Proteobacteria; c__Alphaproteo...\n",
       " ASV496      d__Bacteria; p__Actinobacteriota; c__Actinomyc...\n",
       " \n",
       " [350 rows x 1 columns],\n",
       " 'minus':                                                         Taxon\n",
       " Feature ID                                                   \n",
       " ASV3        d__Bacteria; p__Proteobacteria; c__Alphaproteo...\n",
       " ASV14       d__Bacteria; p__Proteobacteria; c__Alphaproteo...\n",
       " ASV8        d__Bacteria; p__Proteobacteria; c__Alphaproteo...\n",
       " ASV7        d__Bacteria; p__Proteobacteria; c__Alphaproteo...\n",
       " ASV18       d__Bacteria; p__Proteobacteria; c__Alphaproteo...\n",
       " ...                                                       ...\n",
       " ASV485      d__Bacteria; p__Proteobacteria; c__Alphaproteo...\n",
       " ASV491      d__Bacteria; p__Bacteroidota; c__Bacteroidia; ...\n",
       " ASV497      d__Bacteria; p__Bacteroidota; c__Bacteroidia; ...\n",
       " ASV495      d__Bacteria; p__Proteobacteria; c__Alphaproteo...\n",
       " ASV488      d__Bacteria; p__Proteobacteria; c__Alphaproteo...\n",
       " \n",
       " [150 rows x 1 columns]}"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test plug-in here\n",
    "# strand plut passed, both wait debug\n",
    "classification = sintax(query = Artifact.load('/home/navi/synonas/jiedanla/20240513_hjk/dada2_manual/1.Feature_Data_Legacy_Labels/fixed_rep_seqs.qza').view(pd.Series).head(500),\n",
    "           reference_reads = reference_reads.view(pd.Series),\n",
    "           reference_taxonomy = reference_taxonomy.view(pd.DataFrame),\n",
    "           strand = 'both',\n",
    "           threads = \"auto\",\n",
    "           confidence = 0.8\n",
    "           )\n",
    "\n",
    "classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _comp_plus_minus_res_and_opt_final_res(empty_df_w_input_seqs_labs, q2_taxs, q2_tax_rank_splits, id_conf_dfs):\n",
    "    q2_tax = pd.DataFrame(index = empty_df_w_input_seqs_labs.index, columns=['Taxon', 'Confidence']) \n",
    "    # purge 💩 here later...\n",
    "    for index in empty_df_w_input_seqs_labs.index.to_list():\n",
    "        plus_hits_index = q2_taxs['plus'].loc[q2_taxs['plus']['Taxon'] != 'Unclassified'].index\n",
    "        minus_hits_index = q2_taxs['minus'].loc[q2_taxs['minus']['Taxon'] != 'Unclassified'].index\n",
    "        plus_hit = index in plus_hits_index\n",
    "        minus_hit = index in minus_hits_index\n",
    "        if plus_hit and minus_hit:\n",
    "            plus_depth = len(q2_tax_rank_splits['plus'].loc[index, :].dropna())\n",
    "            minus_depth = len(q2_tax_rank_splits['minus'].loc[index, :].dropna())\n",
    "            plus_conf = id_conf_dfs['plus'].at[index, 'Confidence']\n",
    "            minus_conf = id_conf_dfs['minus'].at[index, 'Confidence']\n",
    "            if plus_depth > minus_depth:\n",
    "                q2_tax.at[index, 'Taxon'] = q2_taxs['plus'].at[index, 'Taxon']\n",
    "                conf_assign = plus_conf\n",
    "            elif plus_depth < minus_depth:\n",
    "                q2_tax.at[index, 'Taxon'] = q2_taxs['minus'].at[index, 'Taxon']\n",
    "                conf_assign = minus_conf\n",
    "            else:\n",
    "                if plus_conf > minus_conf:\n",
    "                    q2_tax.at[index, 'Taxon'] = q2_taxs['plus'].at[index, 'Taxon']\n",
    "                    conf_assign = plus_conf\n",
    "                elif plus_conf < minus_conf:\n",
    "                    q2_tax.at[index, 'Taxon'] = q2_taxs['minus'].at[index, 'Taxon']\n",
    "                    conf_assign = minus_conf\n",
    "                else:  # 不会真出回文吧\n",
    "                    q2_tax.at[index, 'Taxon'] = q2_taxs['plus'].at[index, 'Taxon']\n",
    "                    conf_assign = plus_conf\n",
    "        elif plus_hit:\n",
    "            q2_tax.at[index, 'Taxon'] = q2_taxs['plus'].at[index, 'Taxon']\n",
    "            conf_assign = id_conf_dfs['plus'].at[index, 'Confidence']\n",
    "        elif minus_hit:\n",
    "            q2_tax.at[index, 'Taxon'] = q2_taxs['minus'].at[index, 'Taxon']\n",
    "            conf_assign = id_conf_dfs['minus'].at[index, 'Confidence']\n",
    "        else:\n",
    "            q2_tax.at[index, 'Taxon'] = 'Unclassified'\n",
    "            conf_assign = np.nan\n",
    "\n",
    "        q2_tax.at[index, 'Confidence'] = conf_assign\n",
    "\n",
    "    return q2_tax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Taxon</th>\n",
       "      <th>Confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ASV12002</th>\n",
       "      <td>d__Bacteria; p__Proteobacteria; c__Gammaproteo...</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ASV12003</th>\n",
       "      <td>d__Bacteria; p__Proteobacteria; c__Alphaproteo...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ASV12004</th>\n",
       "      <td>d__Bacteria</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ASV12005</th>\n",
       "      <td>d__Bacteria; p__Proteobacteria; c__Gammaproteo...</td>\n",
       "      <td>0.9801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ASV12006</th>\n",
       "      <td>d__Bacteria; p__Bacteroidota; c__Bacteroidia</td>\n",
       "      <td>0.9801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ASV12497</th>\n",
       "      <td>d__Bacteria; p__Proteobacteria; c__Gammaproteo...</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ASV12498</th>\n",
       "      <td>d__Bacteria; p__Proteobacteria; c__Alphaproteo...</td>\n",
       "      <td>0.9801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ASV12499</th>\n",
       "      <td>d__Bacteria; p__Proteobacteria; c__Alphaproteo...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ASV12500</th>\n",
       "      <td>d__Bacteria; p__Proteobacteria; c__Alphaproteo...</td>\n",
       "      <td>0.9124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ASV12501</th>\n",
       "      <td>Unclassified</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      Taxon Confidence\n",
       "ASV12002  d__Bacteria; p__Proteobacteria; c__Gammaproteo...       0.88\n",
       "ASV12003  d__Bacteria; p__Proteobacteria; c__Alphaproteo...        1.0\n",
       "ASV12004                                        d__Bacteria        1.0\n",
       "ASV12005  d__Bacteria; p__Proteobacteria; c__Gammaproteo...     0.9801\n",
       "ASV12006       d__Bacteria; p__Bacteroidota; c__Bacteroidia     0.9801\n",
       "...                                                     ...        ...\n",
       "ASV12497  d__Bacteria; p__Proteobacteria; c__Gammaproteo...       0.83\n",
       "ASV12498  d__Bacteria; p__Proteobacteria; c__Alphaproteo...     0.9801\n",
       "ASV12499  d__Bacteria; p__Proteobacteria; c__Alphaproteo...        1.0\n",
       "ASV12500  d__Bacteria; p__Proteobacteria; c__Alphaproteo...     0.9124\n",
       "ASV12501                                       Unclassified        NaN\n",
       "\n",
       "[500 rows x 2 columns]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_comp_plus_minus_res_and_opt_final_res(empty_df_w_input_seqs_labs_test, q2_taxs_test, q2_tax_rank_splits_test, id_conf_dfs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ASV12002',\n",
       " 'ASV12003',\n",
       " 'ASV12004',\n",
       " 'ASV12005',\n",
       " 'ASV12006',\n",
       " 'ASV12007',\n",
       " 'ASV12008',\n",
       " 'ASV12009',\n",
       " 'ASV12010',\n",
       " 'ASV12011',\n",
       " 'ASV12012',\n",
       " 'ASV12013',\n",
       " 'ASV12014',\n",
       " 'ASV12015',\n",
       " 'ASV12016',\n",
       " 'ASV12017',\n",
       " 'ASV12018',\n",
       " 'ASV12019',\n",
       " 'ASV12020',\n",
       " 'ASV12021',\n",
       " 'ASV12022',\n",
       " 'ASV12023',\n",
       " 'ASV12024',\n",
       " 'ASV12025',\n",
       " 'ASV12026',\n",
       " 'ASV12027',\n",
       " 'ASV12028',\n",
       " 'ASV12029',\n",
       " 'ASV12030',\n",
       " 'ASV12031',\n",
       " 'ASV12032',\n",
       " 'ASV12033',\n",
       " 'ASV12034',\n",
       " 'ASV12035',\n",
       " 'ASV12036',\n",
       " 'ASV12037',\n",
       " 'ASV12038',\n",
       " 'ASV12039',\n",
       " 'ASV12040',\n",
       " 'ASV12041',\n",
       " 'ASV12042',\n",
       " 'ASV12043',\n",
       " 'ASV12044',\n",
       " 'ASV12045',\n",
       " 'ASV12046',\n",
       " 'ASV12047',\n",
       " 'ASV12048',\n",
       " 'ASV12049',\n",
       " 'ASV12050',\n",
       " 'ASV12051',\n",
       " 'ASV12052',\n",
       " 'ASV12053',\n",
       " 'ASV12054',\n",
       " 'ASV12055',\n",
       " 'ASV12056',\n",
       " 'ASV12057',\n",
       " 'ASV12058',\n",
       " 'ASV12059',\n",
       " 'ASV12060',\n",
       " 'ASV12061',\n",
       " 'ASV12062',\n",
       " 'ASV12063',\n",
       " 'ASV12064',\n",
       " 'ASV12065',\n",
       " 'ASV12066',\n",
       " 'ASV12067',\n",
       " 'ASV12068',\n",
       " 'ASV12069',\n",
       " 'ASV12070',\n",
       " 'ASV12071',\n",
       " 'ASV12072',\n",
       " 'ASV12073',\n",
       " 'ASV12074',\n",
       " 'ASV12075',\n",
       " 'ASV12076',\n",
       " 'ASV12077',\n",
       " 'ASV12078',\n",
       " 'ASV12079',\n",
       " 'ASV12080',\n",
       " 'ASV12081',\n",
       " 'ASV12082',\n",
       " 'ASV12083',\n",
       " 'ASV12084',\n",
       " 'ASV12085',\n",
       " 'ASV12086',\n",
       " 'ASV12087',\n",
       " 'ASV12088',\n",
       " 'ASV12089',\n",
       " 'ASV12090',\n",
       " 'ASV12091',\n",
       " 'ASV12092',\n",
       " 'ASV12093',\n",
       " 'ASV12094',\n",
       " 'ASV12095',\n",
       " 'ASV12096',\n",
       " 'ASV12097',\n",
       " 'ASV12098',\n",
       " 'ASV12099',\n",
       " 'ASV12100',\n",
       " 'ASV12101',\n",
       " 'ASV12102',\n",
       " 'ASV12103',\n",
       " 'ASV12104',\n",
       " 'ASV12105',\n",
       " 'ASV12106',\n",
       " 'ASV12107',\n",
       " 'ASV12108',\n",
       " 'ASV12109',\n",
       " 'ASV12110',\n",
       " 'ASV12111',\n",
       " 'ASV12112',\n",
       " 'ASV12113',\n",
       " 'ASV12114',\n",
       " 'ASV12115',\n",
       " 'ASV12116',\n",
       " 'ASV12117',\n",
       " 'ASV12118',\n",
       " 'ASV12119',\n",
       " 'ASV12120',\n",
       " 'ASV12121',\n",
       " 'ASV12122',\n",
       " 'ASV12123',\n",
       " 'ASV12124',\n",
       " 'ASV12125',\n",
       " 'ASV12126',\n",
       " 'ASV12127',\n",
       " 'ASV12128',\n",
       " 'ASV12129',\n",
       " 'ASV12130',\n",
       " 'ASV12131',\n",
       " 'ASV12132',\n",
       " 'ASV12133',\n",
       " 'ASV12134',\n",
       " 'ASV12135',\n",
       " 'ASV12136',\n",
       " 'ASV12137',\n",
       " 'ASV12138',\n",
       " 'ASV12139',\n",
       " 'ASV12140',\n",
       " 'ASV12141',\n",
       " 'ASV12142',\n",
       " 'ASV12143',\n",
       " 'ASV12144',\n",
       " 'ASV12145',\n",
       " 'ASV12146',\n",
       " 'ASV12147',\n",
       " 'ASV12148',\n",
       " 'ASV12149',\n",
       " 'ASV12150',\n",
       " 'ASV12151',\n",
       " 'ASV12152',\n",
       " 'ASV12153',\n",
       " 'ASV12154',\n",
       " 'ASV12155',\n",
       " 'ASV12156',\n",
       " 'ASV12157',\n",
       " 'ASV12158',\n",
       " 'ASV12159',\n",
       " 'ASV12160',\n",
       " 'ASV12161',\n",
       " 'ASV12162',\n",
       " 'ASV12163',\n",
       " 'ASV12164',\n",
       " 'ASV12165',\n",
       " 'ASV12166',\n",
       " 'ASV12167',\n",
       " 'ASV12168',\n",
       " 'ASV12169',\n",
       " 'ASV12170',\n",
       " 'ASV12171',\n",
       " 'ASV12172',\n",
       " 'ASV12173',\n",
       " 'ASV12174',\n",
       " 'ASV12175',\n",
       " 'ASV12176',\n",
       " 'ASV12177',\n",
       " 'ASV12178',\n",
       " 'ASV12179',\n",
       " 'ASV12180',\n",
       " 'ASV12181',\n",
       " 'ASV12182',\n",
       " 'ASV12183',\n",
       " 'ASV12184',\n",
       " 'ASV12185',\n",
       " 'ASV12186',\n",
       " 'ASV12187',\n",
       " 'ASV12188',\n",
       " 'ASV12189',\n",
       " 'ASV12190',\n",
       " 'ASV12191',\n",
       " 'ASV12192',\n",
       " 'ASV12193',\n",
       " 'ASV12194',\n",
       " 'ASV12195',\n",
       " 'ASV12196',\n",
       " 'ASV12197',\n",
       " 'ASV12198',\n",
       " 'ASV12199',\n",
       " 'ASV12200',\n",
       " 'ASV12201',\n",
       " 'ASV12202',\n",
       " 'ASV12203',\n",
       " 'ASV12204',\n",
       " 'ASV12205',\n",
       " 'ASV12206',\n",
       " 'ASV12207',\n",
       " 'ASV12208',\n",
       " 'ASV12209',\n",
       " 'ASV12210',\n",
       " 'ASV12211',\n",
       " 'ASV12212',\n",
       " 'ASV12213',\n",
       " 'ASV12214',\n",
       " 'ASV12215',\n",
       " 'ASV12216',\n",
       " 'ASV12217',\n",
       " 'ASV12218',\n",
       " 'ASV12219',\n",
       " 'ASV12220',\n",
       " 'ASV12221',\n",
       " 'ASV12222',\n",
       " 'ASV12223',\n",
       " 'ASV12224',\n",
       " 'ASV12225',\n",
       " 'ASV12226',\n",
       " 'ASV12227',\n",
       " 'ASV12228',\n",
       " 'ASV12229',\n",
       " 'ASV12230',\n",
       " 'ASV12231',\n",
       " 'ASV12232',\n",
       " 'ASV12233',\n",
       " 'ASV12234',\n",
       " 'ASV12235',\n",
       " 'ASV12236',\n",
       " 'ASV12237',\n",
       " 'ASV12238',\n",
       " 'ASV12239',\n",
       " 'ASV12240',\n",
       " 'ASV12241',\n",
       " 'ASV12242',\n",
       " 'ASV12243',\n",
       " 'ASV12244',\n",
       " 'ASV12245',\n",
       " 'ASV12246',\n",
       " 'ASV12247',\n",
       " 'ASV12248',\n",
       " 'ASV12249',\n",
       " 'ASV12250',\n",
       " 'ASV12251',\n",
       " 'ASV12252',\n",
       " 'ASV12253',\n",
       " 'ASV12254',\n",
       " 'ASV12255',\n",
       " 'ASV12256',\n",
       " 'ASV12257',\n",
       " 'ASV12258',\n",
       " 'ASV12259',\n",
       " 'ASV12260',\n",
       " 'ASV12261',\n",
       " 'ASV12262',\n",
       " 'ASV12263',\n",
       " 'ASV12264',\n",
       " 'ASV12265',\n",
       " 'ASV12266',\n",
       " 'ASV12267',\n",
       " 'ASV12268',\n",
       " 'ASV12269',\n",
       " 'ASV12270',\n",
       " 'ASV12271',\n",
       " 'ASV12272',\n",
       " 'ASV12273',\n",
       " 'ASV12274',\n",
       " 'ASV12275',\n",
       " 'ASV12276',\n",
       " 'ASV12277',\n",
       " 'ASV12278',\n",
       " 'ASV12279',\n",
       " 'ASV12280',\n",
       " 'ASV12281',\n",
       " 'ASV12282',\n",
       " 'ASV12283',\n",
       " 'ASV12284',\n",
       " 'ASV12285',\n",
       " 'ASV12286',\n",
       " 'ASV12287',\n",
       " 'ASV12288',\n",
       " 'ASV12289',\n",
       " 'ASV12290',\n",
       " 'ASV12291',\n",
       " 'ASV12292',\n",
       " 'ASV12293',\n",
       " 'ASV12294',\n",
       " 'ASV12295',\n",
       " 'ASV12296',\n",
       " 'ASV12297',\n",
       " 'ASV12298',\n",
       " 'ASV12299',\n",
       " 'ASV12300',\n",
       " 'ASV12301',\n",
       " 'ASV12302',\n",
       " 'ASV12303',\n",
       " 'ASV12304',\n",
       " 'ASV12305',\n",
       " 'ASV12306',\n",
       " 'ASV12307',\n",
       " 'ASV12308',\n",
       " 'ASV12309',\n",
       " 'ASV12310',\n",
       " 'ASV12311',\n",
       " 'ASV12312',\n",
       " 'ASV12313',\n",
       " 'ASV12314',\n",
       " 'ASV12315',\n",
       " 'ASV12316',\n",
       " 'ASV12317',\n",
       " 'ASV12318',\n",
       " 'ASV12319',\n",
       " 'ASV12320',\n",
       " 'ASV12321',\n",
       " 'ASV12322',\n",
       " 'ASV12323',\n",
       " 'ASV12324',\n",
       " 'ASV12325',\n",
       " 'ASV12326',\n",
       " 'ASV12327',\n",
       " 'ASV12328',\n",
       " 'ASV12329',\n",
       " 'ASV12330',\n",
       " 'ASV12331',\n",
       " 'ASV12332',\n",
       " 'ASV12333',\n",
       " 'ASV12334',\n",
       " 'ASV12335',\n",
       " 'ASV12336',\n",
       " 'ASV12337',\n",
       " 'ASV12338',\n",
       " 'ASV12339',\n",
       " 'ASV12340',\n",
       " 'ASV12341',\n",
       " 'ASV12342',\n",
       " 'ASV12343',\n",
       " 'ASV12344',\n",
       " 'ASV12345',\n",
       " 'ASV12346',\n",
       " 'ASV12347',\n",
       " 'ASV12348',\n",
       " 'ASV12349',\n",
       " 'ASV12350',\n",
       " 'ASV12351',\n",
       " 'ASV12352',\n",
       " 'ASV12353',\n",
       " 'ASV12354',\n",
       " 'ASV12355',\n",
       " 'ASV12356',\n",
       " 'ASV12357',\n",
       " 'ASV12358',\n",
       " 'ASV12359',\n",
       " 'ASV12360',\n",
       " 'ASV12361',\n",
       " 'ASV12362',\n",
       " 'ASV12363',\n",
       " 'ASV12364',\n",
       " 'ASV12365',\n",
       " 'ASV12366',\n",
       " 'ASV12367',\n",
       " 'ASV12368',\n",
       " 'ASV12369',\n",
       " 'ASV12370',\n",
       " 'ASV12371',\n",
       " 'ASV12372',\n",
       " 'ASV12373',\n",
       " 'ASV12374',\n",
       " 'ASV12375',\n",
       " 'ASV12376',\n",
       " 'ASV12377',\n",
       " 'ASV12378',\n",
       " 'ASV12379',\n",
       " 'ASV12380',\n",
       " 'ASV12381',\n",
       " 'ASV12382',\n",
       " 'ASV12383',\n",
       " 'ASV12384',\n",
       " 'ASV12385',\n",
       " 'ASV12386',\n",
       " 'ASV12387',\n",
       " 'ASV12388',\n",
       " 'ASV12389',\n",
       " 'ASV12390',\n",
       " 'ASV12391',\n",
       " 'ASV12392',\n",
       " 'ASV12393',\n",
       " 'ASV12394',\n",
       " 'ASV12395',\n",
       " 'ASV12396',\n",
       " 'ASV12397',\n",
       " 'ASV12398',\n",
       " 'ASV12399',\n",
       " 'ASV12400',\n",
       " 'ASV12401',\n",
       " 'ASV12402',\n",
       " 'ASV12403',\n",
       " 'ASV12404',\n",
       " 'ASV12405',\n",
       " 'ASV12406',\n",
       " 'ASV12407',\n",
       " 'ASV12408',\n",
       " 'ASV12409',\n",
       " 'ASV12410',\n",
       " 'ASV12411',\n",
       " 'ASV12412',\n",
       " 'ASV12413',\n",
       " 'ASV12414',\n",
       " 'ASV12415',\n",
       " 'ASV12416',\n",
       " 'ASV12417',\n",
       " 'ASV12418',\n",
       " 'ASV12419',\n",
       " 'ASV12420',\n",
       " 'ASV12421',\n",
       " 'ASV12422',\n",
       " 'ASV12423',\n",
       " 'ASV12424',\n",
       " 'ASV12425',\n",
       " 'ASV12426',\n",
       " 'ASV12427',\n",
       " 'ASV12428',\n",
       " 'ASV12429',\n",
       " 'ASV12430',\n",
       " 'ASV12431',\n",
       " 'ASV12432',\n",
       " 'ASV12433',\n",
       " 'ASV12434',\n",
       " 'ASV12435',\n",
       " 'ASV12436',\n",
       " 'ASV12437',\n",
       " 'ASV12438',\n",
       " 'ASV12439',\n",
       " 'ASV12440',\n",
       " 'ASV12441',\n",
       " 'ASV12442',\n",
       " 'ASV12443',\n",
       " 'ASV12444',\n",
       " 'ASV12445',\n",
       " 'ASV12446',\n",
       " 'ASV12447',\n",
       " 'ASV12448',\n",
       " 'ASV12449',\n",
       " 'ASV12450',\n",
       " 'ASV12451',\n",
       " 'ASV12452',\n",
       " 'ASV12453',\n",
       " 'ASV12454',\n",
       " 'ASV12455',\n",
       " 'ASV12456',\n",
       " 'ASV12457',\n",
       " 'ASV12458',\n",
       " 'ASV12459',\n",
       " 'ASV12460',\n",
       " 'ASV12461',\n",
       " 'ASV12462',\n",
       " 'ASV12463',\n",
       " 'ASV12464',\n",
       " 'ASV12465',\n",
       " 'ASV12466',\n",
       " 'ASV12467',\n",
       " 'ASV12468',\n",
       " 'ASV12469',\n",
       " 'ASV12470',\n",
       " 'ASV12471',\n",
       " 'ASV12472',\n",
       " 'ASV12473',\n",
       " 'ASV12474',\n",
       " 'ASV12475',\n",
       " 'ASV12476',\n",
       " 'ASV12477',\n",
       " 'ASV12478',\n",
       " 'ASV12479',\n",
       " 'ASV12480',\n",
       " 'ASV12481',\n",
       " 'ASV12482',\n",
       " 'ASV12483',\n",
       " 'ASV12484',\n",
       " 'ASV12485',\n",
       " 'ASV12486',\n",
       " 'ASV12487',\n",
       " 'ASV12488',\n",
       " 'ASV12489',\n",
       " 'ASV12490',\n",
       " 'ASV12491',\n",
       " 'ASV12492',\n",
       " 'ASV12493',\n",
       " 'ASV12494',\n",
       " 'ASV12495',\n",
       " 'ASV12496',\n",
       " 'ASV12497',\n",
       " 'ASV12498',\n",
       " 'ASV12499',\n",
       " 'ASV12500',\n",
       " 'ASV12501']"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty_df_w_input_seqs_labs_test.index.to_list()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qiime2-2023.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
