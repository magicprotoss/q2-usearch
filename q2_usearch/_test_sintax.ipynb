{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiime2 import Artifact\n",
    "\n",
    "reference_reads = Artifact.load('/mnt/amplicon-db/gtdb-207/seqs.qza')\n",
    "reference_taxonomy = Artifact.load('/mnt/amplicon-db/gtdb-207/taxa.qza')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep Final test here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import skbio\n",
    "import subprocess\n",
    "import tempfile\n",
    "import hashlib\n",
    "\n",
    "\n",
    "def run_commands(cmds, verbose=True):\n",
    "    if verbose:\n",
    "        print(\"Running external command line application(s). This may print \"\n",
    "              \"messages to stdout and/or stderr.\")\n",
    "        print(\"The command(s) being run are below. These commands cannot \"\n",
    "              \"be manually re-run as they will depend on temporary files that \"\n",
    "              \"no longer exist.\")\n",
    "    for cmd in cmds:\n",
    "        if verbose:\n",
    "            print(\"\\nCommand:\", end=' ')\n",
    "            print(\" \".join(cmd), end='\\n\\n')\n",
    "        subprocess.run(cmd, check=True)\n",
    "\n",
    "\n",
    "def _get_input_seqs_ids_and_dump_to_fasta(working_dir, query_se):\n",
    "    with open(os.path.join(working_dir, 'query.fasta'), 'wt') as fh:\n",
    "        for index, item in query_se.items():\n",
    "            item.write(fh, format=\"fasta\", max_width=80)\n",
    "    empty_df_w_input_seqs_labs = pd.DataFrame(index=query_se.index)\n",
    "    return empty_df_w_input_seqs_labs\n",
    "\n",
    "\n",
    "def _split_tax_into_ranks(tax, sep):\n",
    "    tax_lst = str(tax).split(sep)\n",
    "    return tax_lst\n",
    "\n",
    "\n",
    "def _split_tax_into_ranks_and_get_max_levels(tax_df_in, sep):\n",
    "    # need to confirm if sintax accepts non-7-rank systems\n",
    "    tax_rank_split_se = tax_df_in['Taxon'].apply(_split_tax_into_ranks, sep=sep)\n",
    "    max_levels = tax_rank_split_se.apply(len).max()\n",
    "    if max_levels > 7:\n",
    "        raise KeyError('according to the doc, sintax only supports up to 7 levels')\n",
    "    return max_levels, tax_rank_split_se\n",
    "\n",
    "\n",
    "def _replace_q2_split_w_usearch_split_and_remove_leading_trailing_blanks(rank_in):\n",
    "    rank_out = re.sub(r\"(?<=\\b[dpcofgs])\\w*__\", ':', str(rank_in).strip())\n",
    "    return rank_out\n",
    "\n",
    "\n",
    "def _replace_non_7bit_ascii_chars(rank_in):\n",
    "    rank_out = rank_in\n",
    "\n",
    "    # let's KISS here...\n",
    "    if re.search(r'[^A-Za-z0-9_.]', rank_in):\n",
    "        if re.match(r'^[kdpcofgs]', str(rank_in).strip()):\n",
    "            rank_out = rank_in[0] + ':' + \\\n",
    "                hashlib.md5(str(rank_in).encode('utf-8')).hexdigest()\n",
    "        else:\n",
    "            rank_out = np.nan\n",
    "    return rank_out\n",
    "\n",
    "\n",
    "def _detect_empty_ph_ranks(rank_in):\n",
    "    result = False\n",
    "    if rank_in == np.nan:\n",
    "        result = True\n",
    "    else:\n",
    "        pattern = r\"^[dpcofgs]:$\"\n",
    "        result = bool(re.match(pattern, rank_in))\n",
    "    return result\n",
    "\n",
    "\n",
    "def _join_levels_for_usearch(tax_series_in):\n",
    "    # This pile of ðŸ’© is here for one reason:\n",
    "    # if a non authoritive database contains a place holder in a parent rank and also have a non-empty child rank\n",
    "    usearch_tax_anno_str = ';tax='\n",
    "    for level, (index, rank) in enumerate(tax_series_in.items(), 1):\n",
    "        detection_res = _detect_empty_ph_ranks(rank)\n",
    "        if detection_res:\n",
    "            break\n",
    "        else:\n",
    "            if level == 1:\n",
    "                usearch_tax_anno_str = usearch_tax_anno_str + rank\n",
    "            else:\n",
    "                usearch_tax_anno_str = usearch_tax_anno_str + ',' + rank\n",
    "    usearch_tax_anno_str = usearch_tax_anno_str + ';'\n",
    "    return usearch_tax_anno_str\n",
    "\n",
    "\n",
    "def _make_tmp_tax_mapping_df(tax_df_in):\n",
    "\n",
    "    tax_df_out = tax_df_in.copy()\n",
    "\n",
    "    max_level, tax_rank_split_se = _split_tax_into_ranks_and_get_max_levels(\n",
    "        tax_df_in, ';')\n",
    "\n",
    "    ori_tax_cols_lst = ['q2_' + 'level' + '_' + str(i) for i in range(1, max_level + 1)]\n",
    "    u_tax_cols_lst = ['usearch_' + 'level' + '_' +\n",
    "                      str(i) for i in range(1, max_level + 1)]\n",
    "\n",
    "    # ðŸ’© super slow but works\n",
    "    tax_df_out[ori_tax_cols_lst] = tax_rank_split_se.apply(pd.Series)\n",
    "\n",
    "    tax_df_out[u_tax_cols_lst] = tax_df_out[ori_tax_cols_lst]\n",
    "\n",
    "    for u_level in u_tax_cols_lst:\n",
    "        tax_df_out[u_level] = tax_df_out[u_level].apply(\n",
    "            _replace_q2_split_w_usearch_split_and_remove_leading_trailing_blanks)\n",
    "        tax_df_out[u_level] = tax_df_out[u_level].apply(_replace_non_7bit_ascii_chars)\n",
    "    tax_df_out['usearch_tax'] = tax_df_out[u_tax_cols_lst].apply(\n",
    "        _join_levels_for_usearch, axis=1)\n",
    "\n",
    "    return tax_df_out\n",
    "\n",
    "\n",
    "def _convert_q2_seqs_and_taxa_to_utax(working_dir, reference_reads, reference_taxonomy, verbose):\n",
    "    ref_reads_se = reference_reads\n",
    "    ref_reads_se.name = 'Seqs'\n",
    "    ref_reads_se.index.name = 'Feature ID'\n",
    "    ref_taxa_df = reference_taxonomy\n",
    "    # check if dumping tax_df to pickle is nessesary with low spec pcs\n",
    "    # silva 138.1 only took 72m mem, no need here\n",
    "    if verbose:\n",
    "        print(\"Building usearch compatible fasta db file, this could take a while...\")\n",
    "\n",
    "    tmp_taxa_map_df = _make_tmp_tax_mapping_df(ref_taxa_df)\n",
    "\n",
    "    op_fa = os.path.join(working_dir, 'ref_seqs_tax.fa')\n",
    "    with open(op_fa, 'wt') as fh:\n",
    "        for index, item in ref_reads_se.items():\n",
    "            tax_info = tmp_taxa_map_df.at[index, 'usearch_tax']\n",
    "            seq_to_dump = skbio.DNA(str(item).upper(), metadata={\n",
    "                                    'id': index + tax_info})\n",
    "            seq_to_dump.write(fh, format=\"fasta\", max_width=80)\n",
    "\n",
    "    return tmp_taxa_map_df\n",
    "\n",
    "\n",
    "def _build_udb():\n",
    "    # seemed unnessasary, sintax builds one on the fly, and tmp dirs don't presist in a q2 pipeline\n",
    "    pass\n",
    "\n",
    "\n",
    "def _run_sintax(working_dir, query_seqs_fp, strand, threads, verbose):\n",
    "    # build sintax command\n",
    "    cmd = ['usearch', '-sintax', query_seqs_fp, '-db', os.path.join(\n",
    "        working_dir, 'ref_seqs_tax.fa'), '-tabbedout', os.path.join(working_dir, 'sintax.tsv')]\n",
    "\n",
    "    if strand == 'plus':\n",
    "        cmd += ['-strand', 'plus']\n",
    "    else:\n",
    "        cmd += ['-strand', 'both']\n",
    "\n",
    "    if threads != 1:\n",
    "        cmd += ['-threads', str(threads)]\n",
    "\n",
    "    run_commands([cmd])\n",
    "\n",
    "\n",
    "def _rm_conf_value_and_trim_fp_ranks(x, cut_off=float):\n",
    "    if x == None:\n",
    "        x_str = np.nan\n",
    "    else:\n",
    "        sintan_conf_col_loci = x.rfind('(')\n",
    "        x_str = x[:sintan_conf_col_loci]\n",
    "        x_conf = float(x[sintan_conf_col_loci + 1:].replace(')', ''))\n",
    "        if x_conf < cut_off:\n",
    "            x_str = np.nan\n",
    "    return x_str\n",
    "\n",
    "\n",
    "def _get_conf_value_deepest_rank(se_in, cut_off=float):\n",
    "\n",
    "    conf = np.nan\n",
    "    for index, item in se_in.items():\n",
    "        sintan_conf_col_loci = item.rfind('(')\n",
    "        conf_item = float(item[sintan_conf_col_loci + 1:].replace(')', ''))\n",
    "        if conf_item < cut_off:\n",
    "            break\n",
    "        conf = conf_item\n",
    "\n",
    "    return conf\n",
    "\n",
    "\n",
    "def _split_utax_and_get_conf_lr(usearch_tax_df_in, confidence):\n",
    "\n",
    "    tax_rank_split_df = pd.DataFrame(index=usearch_tax_df_in.index)\n",
    "\n",
    "    max_level, usearch_tax_rank_split_df = _split_tax_into_ranks_and_get_max_levels(\n",
    "        usearch_tax_df_in, ',')\n",
    "\n",
    "    u_tax_cols_lst = ['usearch_' + 'level' + '_' +\n",
    "                      str(i) for i in range(1, max_level + 1)]\n",
    "\n",
    "    # ðŸ’© super slow but works\n",
    "    tax_rank_split_df[u_tax_cols_lst] = usearch_tax_rank_split_df.apply(pd.Series)\n",
    "\n",
    "    id_conf_df = pd.DataFrame(index=usearch_tax_df_in.index)\n",
    "\n",
    "    for index, row in tax_rank_split_df.iterrows():\n",
    "        id_conf_df.at[index, 'Confidence'] = _get_conf_value_deepest_rank(\n",
    "            row, confidence)\n",
    "\n",
    "    tax_rank_split_df = tax_rank_split_df.applymap(\n",
    "        _rm_conf_value_and_trim_fp_ranks, cut_off=confidence)\n",
    "\n",
    "    return tax_rank_split_df, id_conf_df\n",
    "\n",
    "\n",
    "def _map_utax_to_q2_tax(tax_rank_split_in, taxa_map_df):\n",
    "    # split input and map into sep dfs, left join and concat back\n",
    "    levels = len(tax_rank_split_in.columns)\n",
    "    tax_rank_split_mapped_to_q2_tax = tax_rank_split_in.copy()\n",
    "    for i in range(1, levels + 1):\n",
    "        key = \"level_\" + str(i)\n",
    "        taxa_map_df_sub_level = taxa_map_df[['usearch_' + key, 'q2_' + key]]\n",
    "        taxa_map_df_sub_level_uni = taxa_map_df_sub_level.drop_duplicates().dropna()\n",
    "        taxa_map_df_sub_level_uni_dict = taxa_map_df_sub_level_uni.set_index(\n",
    "            'usearch_' + key).iloc[:, 0].to_dict()\n",
    "        tax_rank_split_mapped_to_q2_tax['usearch_' + key] = tax_rank_split_in['usearch_' + key].replace(\n",
    "            taxa_map_df_sub_level_uni_dict)\n",
    "    return tax_rank_split_mapped_to_q2_tax\n",
    "\n",
    "\n",
    "def _join_q2_tax(q2_tax_rank_split_in):\n",
    "    q2_tax = pd.DataFrame()\n",
    "    for index, row in q2_tax_rank_split_in.iterrows():\n",
    "        tax_str = '; '.join(row.dropna().values)\n",
    "        if len(tax_str) == 0:\n",
    "            tax_str = 'Unclassified'\n",
    "        tmp_df = pd.DataFrame({'Taxon': {index: tax_str}})\n",
    "        q2_tax = pd.concat([q2_tax, tmp_df])\n",
    "    q2_tax.index.name = 'Feature ID'\n",
    "\n",
    "    return q2_tax\n",
    "\n",
    "\n",
    "def _comp_plus_minus_res_and_opt_final_res(empty_df_w_input_seqs_labs, q2_taxs, q2_tax_rank_splits, id_conf_dfs):\n",
    "    q2_tax = pd.DataFrame(index=empty_df_w_input_seqs_labs.index,\n",
    "                          columns=['Taxon', 'Confidence'])\n",
    "    # purge ðŸ’© here later...\n",
    "    for index in empty_df_w_input_seqs_labs.index.to_list():\n",
    "        plus_hits_index = q2_taxs['plus'].loc[q2_taxs['plus']\n",
    "                                              ['Taxon'] != 'Unclassified'].index\n",
    "        minus_hits_index = q2_taxs['minus'].loc[q2_taxs['minus']\n",
    "                                                ['Taxon'] != 'Unclassified'].index\n",
    "        plus_hit = index in plus_hits_index\n",
    "        minus_hit = index in minus_hits_index\n",
    "        if plus_hit and minus_hit:\n",
    "            plus_depth = len(q2_tax_rank_splits['plus'].loc[index, :].dropna())\n",
    "            minus_depth = len(q2_tax_rank_splits['minus'].loc[index, :].dropna())\n",
    "            plus_conf = id_conf_dfs['plus'].at[index, 'Confidence']\n",
    "            minus_conf = id_conf_dfs['minus'].at[index, 'Confidence']\n",
    "            if plus_depth > minus_depth:\n",
    "                q2_tax.at[index, 'Taxon'] = q2_taxs['plus'].at[index, 'Taxon']\n",
    "                conf_assign = plus_conf\n",
    "            elif plus_depth < minus_depth:\n",
    "                q2_tax.at[index, 'Taxon'] = q2_taxs['minus'].at[index, 'Taxon']\n",
    "                conf_assign = minus_conf\n",
    "            else:\n",
    "                if plus_conf > minus_conf:\n",
    "                    q2_tax.at[index, 'Taxon'] = q2_taxs['plus'].at[index, 'Taxon']\n",
    "                    conf_assign = plus_conf\n",
    "                elif plus_conf < minus_conf:\n",
    "                    q2_tax.at[index, 'Taxon'] = q2_taxs['minus'].at[index, 'Taxon']\n",
    "                    conf_assign = minus_conf\n",
    "                else:  # ä¸ä¼šçœŸå‡ºå›žæ–‡å§\n",
    "                    q2_tax.at[index, 'Taxon'] = q2_taxs['plus'].at[index, 'Taxon']\n",
    "                    conf_assign = plus_conf\n",
    "        elif plus_hit:\n",
    "            q2_tax.at[index, 'Taxon'] = q2_taxs['plus'].at[index, 'Taxon']\n",
    "            conf_assign = id_conf_dfs['plus'].at[index, 'Confidence']\n",
    "        elif minus_hit:\n",
    "            q2_tax.at[index, 'Taxon'] = q2_taxs['minus'].at[index, 'Taxon']\n",
    "            conf_assign = id_conf_dfs['minus'].at[index, 'Confidence']\n",
    "        else:\n",
    "            q2_tax.at[index, 'Taxon'] = 'Unclassified'\n",
    "            conf_assign = np.nan\n",
    "\n",
    "        q2_tax.at[index, 'Confidence'] = conf_assign\n",
    "\n",
    "    return q2_tax\n",
    "\n",
    "\n",
    "def _collect_sintax_anno_to_q2_anno(working_dir, taxa_map_df, empty_df_w_input_seqs_labs, strand, confidence, verbose):\n",
    "\n",
    "    # read sintax res into pd.DataFrame\n",
    "    usearch_tax = pd.read_csv(os.path.join(\n",
    "        working_dir, 'sintax.tsv'), sep='\\t', header=None, index_col=0)\n",
    "\n",
    "    usearch_tax.index.name = 'Feature ID'\n",
    "\n",
    "    usearch_tax = usearch_tax.iloc[:, 0:2]\n",
    "\n",
    "    usearch_tax.columns = ['Taxon', 'Strand']\n",
    "\n",
    "    # rewrite in a less ðŸ’© way here later...\n",
    "    if strand != 'plus':\n",
    "        usearch_tax_both = {}\n",
    "        usearch_tax_both['plus'] = usearch_tax.loc[usearch_tax['Strand']\n",
    "                                                   == '+', 'Taxon'].to_frame()\n",
    "        usearch_tax_both['minus'] = usearch_tax.loc[usearch_tax['Strand']\n",
    "                                                    == '-', 'Taxon'].to_frame()\n",
    "        usearch_tax_rank_split_dfs = {}\n",
    "        id_conf_dfs = {}\n",
    "        q2_tax_rank_splits = {}\n",
    "        q2_taxs = {}\n",
    "        for key, value in usearch_tax_both.items():\n",
    "\n",
    "            usearch_tax_rank_split_dfs[key], id_conf_dfs[key] = _split_utax_and_get_conf_lr(\n",
    "                value, confidence)\n",
    "\n",
    "            q2_tax_rank_splits[key] = _map_utax_to_q2_tax(\n",
    "                usearch_tax_rank_split_dfs[key], taxa_map_df)\n",
    "\n",
    "            q2_taxs[key] = _join_q2_tax(q2_tax_rank_splits[key])\n",
    "\n",
    "        q2_tax = _comp_plus_minus_res_and_opt_final_res(\n",
    "            empty_df_w_input_seqs_labs, q2_taxs, q2_tax_rank_splits, id_conf_dfs)\n",
    "\n",
    "        # purge ðŸ’© here later...\n",
    "        for index in empty_df_w_input_seqs_labs.index:\n",
    "            plus_hit = index in q2_taxs['plus'].index\n",
    "            minus_hit = index in q2_taxs['minus'].index\n",
    "            if plus_hit and minus_hit:\n",
    "                plus_depth = len(q2_tax_rank_splits['plus'].loc[index, :].dropna())\n",
    "                minus_depth = len(q2_tax_rank_splits['minus'].loc[index, :].dropna())\n",
    "                plus_conf = id_conf_dfs['plus'].at[index, 'Confidence']\n",
    "                minus_conf = id_conf_dfs['minus'].at[index, 'Confidence']\n",
    "                if plus_depth > minus_depth:\n",
    "                    q2_tax.at[index, 'Taxon'] = q2_taxs['plus'].at[index, 'Taxon']\n",
    "                    conf_assign = plus_conf\n",
    "                elif plus_depth < minus_depth:\n",
    "                    q2_tax.at[index, 'Taxon'] = q2_taxs['minus'].at[index, 'Taxon']\n",
    "                    conf_assign = minus_conf\n",
    "                else:\n",
    "                    if plus_conf > minus_conf:\n",
    "                        q2_tax.at[index, 'Taxon'] = q2_taxs['plus'].at[index, 'Taxon']\n",
    "                        conf_assign = plus_conf\n",
    "                    elif plus_conf < minus_conf:\n",
    "                        q2_tax.at[index, 'Taxon'] = q2_taxs['minus'].at[index, 'Taxon']\n",
    "                        conf_assign = minus_conf\n",
    "                    else:  # ä¸ä¼šçœŸå‡ºå›žæ–‡å§\n",
    "                        q2_tax.at[index, 'Taxon'] = q2_taxs['plus'].at[index, 'Taxon']\n",
    "                        conf_assign = plus_conf\n",
    "            elif plus_hit:\n",
    "                q2_tax.at[index, 'Taxon'] = q2_taxs['plus'].at[index, 'Taxon']\n",
    "                conf_assign = id_conf_dfs['plus'].at[index, 'Confidence']\n",
    "            elif minus_hit:\n",
    "                q2_tax.at[index, 'Taxon'] = q2_taxs['minus'].at[index, 'Taxon']\n",
    "                conf_assign = id_conf_dfs['minus'].at[index, 'Confidence']\n",
    "            else:\n",
    "                q2_tax.at[index, 'Taxon'] = 'Unclassified'\n",
    "                conf_assign = np.nan\n",
    "\n",
    "            q2_tax.at[index, 'Confidence'] = conf_assign\n",
    "\n",
    "    else:\n",
    "        usearch_tax = usearch_tax['Taxon'].to_frame()\n",
    "\n",
    "        usearch_tax_rank_split_df, id_conf_df = _split_utax_and_get_conf_lr(\n",
    "            usearch_tax, confidence)\n",
    "\n",
    "        q2_tax_rank_split = _map_utax_to_q2_tax(usearch_tax_rank_split_df, taxa_map_df)\n",
    "\n",
    "        q2_tax = _join_q2_tax(q2_tax_rank_split)\n",
    "\n",
    "        q2_tax = pd.merge(q2_tax, id_conf_df, left_index=True,\n",
    "                          right_index=True, how='inner')\n",
    "\n",
    "        q2_tax = pd.merge(empty_df_w_input_seqs_labs, q2_tax,\n",
    "                          left_index=True, right_index=True, how='left')\n",
    "\n",
    "        for index, row in q2_tax.iterrows():\n",
    "            if row['Taxon'] is None:\n",
    "                q2_tax.at[index, 'Taxon'] == 'Unclassified'\n",
    "\n",
    "    q2_tax.index.name = 'Feature ID'\n",
    "\n",
    "    return q2_tax\n",
    "\n",
    "\n",
    "def sintax(query: pd.Series,\n",
    "           reference_reads: pd.Series,\n",
    "           reference_taxonomy: pd.DataFrame,\n",
    "           # limited test suggest it's common for sintax to report a better match in rev-comp using plus only 16s as input\n",
    "           # maybe throw in orinet as a precaution? warn user?\n",
    "           strand: str = 'plus',\n",
    "           threads: str = \"auto\",\n",
    "           confidence: float = 0.8\n",
    "           ) -> pd.DataFrame:\n",
    "\n",
    "    verbose = True\n",
    "\n",
    "    if threads == \"auto\":\n",
    "        threads = os.cpu_count() - 3\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as usearch_wd:\n",
    "\n",
    "        empty_df_w_input_seqs_labs = _get_input_seqs_ids_and_dump_to_fasta(\n",
    "            usearch_wd, query)\n",
    "\n",
    "        taxa_map_df = _convert_q2_seqs_and_taxa_to_utax(\n",
    "            usearch_wd, reference_reads, reference_taxonomy, verbose)\n",
    "\n",
    "        query_fp = os.path.join(usearch_wd, 'query.fasta')\n",
    "\n",
    "        _run_sintax(usearch_wd, query_fp, strand, threads, verbose)\n",
    "\n",
    "        classification = _collect_sintax_anno_to_q2_anno(\n",
    "            usearch_wd, taxa_map_df, empty_df_w_input_seqs_labs, strand, confidence, verbose)\n",
    "\n",
    "    return classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building usearch compatible fasta db file, this could take a while...\n",
      "Running external command line application(s). This may print messages to stdout and/or stderr.\n",
      "The command(s) being run are below. These commands cannot be manually re-run as they will depend on temporary files that no longer exist.\n",
      "\n",
      "Command: usearch -sintax /tmp/tmpm1x9gyd8/query.fasta -db /tmp/tmpm1x9gyd8/ref_seqs_tax.fa -tabbedout /tmp/tmpm1x9gyd8/sintax.tsv -strand plus -threads 21\n",
      "\n",
      "usearch v11.0.667_i86linux64, 132Gb RAM, 24 cores\n",
      "(C) Copyright 2013-18 Robert C. Edgar, all rights reserved.\n",
      "https://drive5.com/usearch\n",
      "\n",
      "License: yxliu@genetics.ac.cn, non-profit use, max 1 process(es)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:00 108Mb   100.0% Reading /tmp/tmpm1x9gyd8/ref_seqs_tax.fa\n",
      "00:01 74Mb    100.0% Masking (fastnucleo)                    \n",
      "00:02 75Mb    100.0% Word stats          \n",
      "00:02 75Mb    100.0% Alloc rows\n",
      "00:04 269Mb   100.0% Build index\n",
      "00:05 1.8Gb   100.0% Processing \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Taxon</th>\n",
       "      <th>Confidence</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feature ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ASV1</th>\n",
       "      <td>d__Bacteria; p__Proteobacteria; c__Alphaproteo...</td>\n",
       "      <td>0.9000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ASV2</th>\n",
       "      <td>d__Bacteria; p__Proteobacteria; c__Alphaproteo...</td>\n",
       "      <td>0.8500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ASV3</th>\n",
       "      <td>d__Bacteria; p__Proteobacteria; c__Alphaproteo...</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ASV4</th>\n",
       "      <td>d__Bacteria; p__Proteobacteria; c__Alphaproteo...</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ASV5</th>\n",
       "      <td>d__Bacteria; p__Bacteroidota; c__Bacteroidia; ...</td>\n",
       "      <td>0.8500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ASV496</th>\n",
       "      <td>d__Bacteria; p__Actinobacteriota; c__Actinomyc...</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ASV497</th>\n",
       "      <td>d__Bacteria; p__Bacteroidota; c__Bacteroidia; ...</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ASV498</th>\n",
       "      <td>d__Bacteria; p__Proteobacteria; c__Alphaproteo...</td>\n",
       "      <td>0.8100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ASV499</th>\n",
       "      <td>d__Bacteria; p__Proteobacteria; c__Alphaproteo...</td>\n",
       "      <td>0.9800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ASV500</th>\n",
       "      <td>d__Bacteria; p__Bacteroidota; c__Bacteroidia; ...</td>\n",
       "      <td>0.8811</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        Taxon  Confidence\n",
       "Feature ID                                                               \n",
       "ASV1        d__Bacteria; p__Proteobacteria; c__Alphaproteo...      0.9000\n",
       "ASV2        d__Bacteria; p__Proteobacteria; c__Alphaproteo...      0.8500\n",
       "ASV3        d__Bacteria; p__Proteobacteria; c__Alphaproteo...      1.0000\n",
       "ASV4        d__Bacteria; p__Proteobacteria; c__Alphaproteo...      1.0000\n",
       "ASV5        d__Bacteria; p__Bacteroidota; c__Bacteroidia; ...      0.8500\n",
       "...                                                       ...         ...\n",
       "ASV496      d__Bacteria; p__Actinobacteriota; c__Actinomyc...      1.0000\n",
       "ASV497      d__Bacteria; p__Bacteroidota; c__Bacteroidia; ...      1.0000\n",
       "ASV498      d__Bacteria; p__Proteobacteria; c__Alphaproteo...      0.8100\n",
       "ASV499      d__Bacteria; p__Proteobacteria; c__Alphaproteo...      0.9800\n",
       "ASV500      d__Bacteria; p__Bacteroidota; c__Bacteroidia; ...      0.8811\n",
       "\n",
       "[500 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test plug-in here\n",
    "# strand plut passed, both wait debug\n",
    "classification = sintax(query = Artifact.load('/home/navi/synonas/jiedanla/20240513_hjk/dada2_manual/1.Feature_Data_Legacy_Labels/fixed_rep_seqs.qza').view(pd.Series).head(500),\n",
    "           reference_reads = reference_reads.view(pd.Series),\n",
    "           reference_taxonomy = reference_taxonomy.view(pd.DataFrame),\n",
    "           strand = 'plus',\n",
    "           threads = \"auto\",\n",
    "           confidence = 0.8\n",
    "           )\n",
    "\n",
    "classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification.index.name == 'Feature ID'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qiime2-2023.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
